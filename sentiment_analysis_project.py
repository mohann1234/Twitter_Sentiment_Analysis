# -*- coding: utf-8 -*-
"""Sentiment Analysis project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VQk7AEmYdEBcn7WbPZ4_-DAdQGm3T1kb
"""

from google.colab import files
files.upload()  # Upload kaggle.json

!mkdir -p ~/.kaggle
!cp "kaggle (1) (1).json" ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d kazanova/sentiment140
!unzip sentiment140.zip

import pandas as pd

df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)
display(df.head())

import pandas as pd

# Load raw CSV
df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header=None)
df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']

# Keep only text and sentiment
df = df[['text', 'sentiment']]

# Only binary sentiment for simplicity: 0 = Negative, 4 = Positive
df = df[df['sentiment'] != 2]
df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})

# Sample small data for faster training
df = df.sample(10000, random_state=42).reset_index(drop=True)
df.head()

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def clean_tweet(tweet):
    tweet = tweet.lower()
    tweet = re.sub(r"http\S+|@\S+|#[A-Za-z0-9_]+", "", tweet)  # remove links, mentions, hashtags
    tweet = re.sub(r"[^a-z\s]", "", tweet)  # remove punctuation
    words = tweet.split()
    words = [w for w in words if w not in stop_words]
    return " ".join(words)

df['clean_text'] = df['text'].apply(clean_tweet)

from collections import Counter
import numpy as np

# Build vocabulary
def build_vocab(texts, max_words=2000):
    all_words = []
    for text in texts:
        all_words.extend(text.split())
    most_common = Counter(all_words).most_common(max_words)
    return {word: i for i, (word, _) in enumerate(most_common)}

vocab = build_vocab(df['clean_text'])

# Vectorize a sentence
def vectorize(text, vocab):
    vec = np.zeros(len(vocab))
    for word in text.split():
        if word in vocab:
            vec[vocab[word]] += 1
    return vec

X = np.array([vectorize(text, vocab) for text in df['clean_text']])
y = df['sentiment'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Sigmoid and loss functions
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def predict(X, weights):
    return sigmoid(np.dot(X, weights))

def compute_loss(X, y, weights):
    m = X.shape[0]
    h = predict(X, weights)
    return -1/m * np.sum(y * np.log(h + 1e-10) + (1 - y) * np.log(1 - h + 1e-10))

# Initialize weights
weights = np.zeros(X_train.shape[1])
lr = 0.01
epochs = 1000

for epoch in range(epochs):
    h = predict(X_train, weights)
    gradient = np.dot(X_train.T, (h - y_train)) / y_train.size
    weights -= lr * gradient

    if epoch % 100 == 0:
        loss = compute_loss(X_train, y_train, weights)
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Predict on test
y_pred_prob = predict(X_test, weights)
y_pred = (y_pred_prob >= 0.5).astype(int)

# Accuracy
accuracy = np.mean(y_pred == y_test)
print("Test Accuracy:", accuracy)

def predict_custom(tweet):
    tweet_clean = clean_tweet(tweet)
    vec = vectorize(tweet_clean, vocab).reshape(1, -1)
    prob = sigmoid(np.dot(vec, weights))[0]
    sentiment = "Positive ðŸ˜Š" if prob >= 0.5 else "Negative ðŸ˜ž"
    print(f"Tweet: {tweet}\nPredicted Sentiment: {sentiment}")

predict_custom("I love this product. It's awesome!")
predict_custom("This is the worst thing I've ever bought.")

import matplotlib.pyplot as plt

def plot_sentiment_distribution(y_data):
    labels = ['Negative', 'Positive']
    values = [sum(y_data == 0), sum(y_data == 1)]

    plt.figure(figsize=(5, 4))
    plt.bar(labels, values, color=['red', 'green'])
    plt.title("Sentiment Distribution in Sampled Data")
    plt.xlabel("Sentiment")
    plt.ylabel("Number of Tweets")
    plt.show()

plot_sentiment_distribution(y)

from wordcloud import WordCloud

def show_wordcloud(texts, title):
    text = " ".join(texts)
    wc = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

# Show for positive and negative tweets
show_wordcloud(df[df['sentiment'] == 1]['clean_text'], "Positive Tweets WordCloud")
show_wordcloud(df[df['sentiment'] == 0]['clean_text'], "Negative Tweets WordCloud")

def predict_custom_with_confidence(tweet):
    tweet_clean = clean_tweet(tweet)
    vec = vectorize(tweet_clean, vocab).reshape(1, -1)
    prob = sigmoid(np.dot(vec, weights))[0]
    sentiment = "Positive ðŸ˜Š" if prob >= 0.5 else "Negative ðŸ˜ž"
    confidence = prob if prob >= 0.5 else 1 - prob
    print(f"Tweet: {tweet}\nSentiment: {sentiment} ({confidence*100:.2f}% confidence)")

predict_custom_with_confidence("I love this!")
predict_custom_with_confidence("Absolutely hated it.")